{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3793abb1",
   "metadata": {},
   "source": [
    "# Implementing a Neural Network From Scratch\n",
    "### This notebook is a personal exercise in implementing a Neural Netowork from scratch (i.e., no frameworks).\n",
    "### I intend to explain as many of specifics as I can, primarily to educate myself, and secondarily to anyone who views this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6696e7",
   "metadata": {},
   "source": [
    "# Part 1 - Importing, Exploring, and Preparing the Data\n",
    "### This notebook uses the MNIST dataset to classify hand-written digits. Typically image classification is done with Convolutional Neural Networks, but these images are simple enough that processing with a standard neural network is feasible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6bc3bb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the relevant libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tensorflow.keras.datasets import mnist # This is the dataset, not using any frameworks in this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4de814da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Data\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "X_tr, Y_tr, X_tst, Y_tst = X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "738d4f27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    This image is labeled as a \"9\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualize an Example, any example\n",
    "ex_img = 4\n",
    "image = X_train[ex_img]\n",
    "fig = plt.figure\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()\n",
    "print(f'    This image is labeled as a \"{Y_train[ex_img]}\"\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08b99eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each image is of type: <class 'numpy.ndarray'>\n",
      "\n",
      "Each image is of size: (28, 28)\n",
      "\n",
      "There are 60000 example images in X_train, with 60000 labels.\n",
      "\n",
      "X_train is of shape (60000, 28, 28) and Y_train is of shape (60000,).\n",
      "\n",
      "The minimum value in an array is 0, while the max is 255. (Typical for color values)\n"
     ]
    }
   ],
   "source": [
    "# Learn about the data\n",
    "print(f'Each image is of type: {type(image)}\\n')\n",
    "print(f'Each image is of size: {np.shape(image)}\\n')\n",
    "print(f'There are {len(X_train)} example images in X_train, with {len(Y_train)} labels.\\n')\n",
    "print(f'X_train is of shape {np.shape(X_train)} and Y_train is of shape {np.shape(Y_train)}.\\n')\n",
    "print(f'The minimum value in an array is {np.min(X_train)}, while the max is {np.max(X_train)}. (Typical for color values)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f611755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array of flattened images is of shape (10000, 784).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Flatten each of the images so each image is a single row (i.e, the array will be of shape (60,000, 28x28)).\n",
    "X_train = X_train.reshape(len(X_train), -1)\n",
    "X_test = X_test.reshape(len(X_test), -1)\n",
    "print(f'The array of flattened images is of shape {np.shape(X_test)}.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51477083",
   "metadata": {},
   "source": [
    "### Data should be standardized or normalized before it can be processed. There are many different methods depending on the type of data. Here, since the values range from 0 to 255, dividing all values by 255 will ensure that all values are between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33dc6f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "# transpose this matrix so it will be able to be used in a dot-product\n",
    "X_train = np.transpose(X_train)\n",
    "X_test = np.transpose(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b44d4d",
   "metadata": {},
   "source": [
    "### Converting the labels to one-hot encoded vectors is necessary for the Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c594c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of \"y_train_oh\" is (10, 60000).\n",
      "\n",
      "The first several one-hot encode vectors are printed below:\n",
      "\n",
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Turn each value of the labels into a one-hot vector and transpose.\n",
    "Y_train_oh = np.zeros((np.shape(Y_train)[0], np.max(Y_train) + 1))\n",
    "Y_train_oh[np.arange(len(Y_train)), Y_train] = 1\n",
    "Y_train_oh = np.transpose(Y_train_oh)\n",
    "\n",
    "Y_test_oh = np.zeros((np.shape(Y_test)[0], np.max(Y_test) + 1))\n",
    "Y_test_oh[np.arange(len(Y_test)), Y_test] = 1\n",
    "Y_test_oh = np.transpose(Y_test_oh)\n",
    "\n",
    "# Transpose matrix\n",
    "print(f'The shape of \"y_train_oh\" is {np.shape(Y_train_oh)}.\\n')\n",
    "print('The first several one-hot encode vectors are printed below:\\n')\n",
    "print(Y_train_oh[:, 0:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67aa4e85",
   "metadata": {},
   "source": [
    "# Part 2 - Initializing the Parameters and Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83258ddf",
   "metadata": {},
   "source": [
    "### I will be creating a Neural Network with 2 hidden layers, which means there will be 3 layers when the output layer is included. Apparently by convention, the input layer does not count as a \"layer\", but I will define it as layer \"0\". The the two hidden layers can have any number of nodes (within reason). The final layer will have 10 nodes, which is chosen since there are 10 output classes (10 digits, 0-9)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b4637d",
   "metadata": {},
   "source": [
    "### Here I create a function that initializes the weight and bias parameters \n",
    "#### Random initialization is necessary for the weights in order to \"break symmetry\". Without this, each node will be computing \"0\", meaning no node will be favored by the network over others, and the network will be \"learning\" nothing.\n",
    "#### Learning Note: I learned from this experience that initialization is more important than I originally conceived. First, I used np.random.rand, which initializes with numbers between 0 and 1. This creates problems later in the process, as computation values will explode, and quickly become unworkable (python starts returning 'nan'). I switched to using np.random.randn, which uses a normal distribution to initialize. I also included a scaling factor, which makes my initialization a method called \"He Initialization\". This works if reLU is used for the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1808a59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(nodes):\n",
    "    \"\"\"Initialize the weight matrices and bias vectors\"\"\"\n",
    "    \"\"\"\n",
    "    Inputs: \n",
    "            nodes: vector with the number of nodes in layer[i]\n",
    "            \n",
    "    Outputs:\n",
    "            Weights array - array of arrays that hold the weight values for each layer\n",
    "            Bias array - array of vectors that hold the bias values for each layer\n",
    "    \"\"\"\n",
    "    \n",
    "        # initialize vector to hold weights and bias\n",
    "    layer_weights = []\n",
    "    layer_biases = []\n",
    "        \n",
    "        # Loop through 'nodes' to initialize the weights and biases in layer[i]\n",
    "    for i in range(len(nodes)):\n",
    "        if i == 0: # No weights are needed for layer \"0\" - this is my way of making sure that weights[i] correspond with layer[i]\n",
    "            weights = 'NaN'\n",
    "            b = 'NaN'\n",
    "        else:\n",
    "            # randn should be used, as this creates positive and negative starting weights\n",
    "            # This method (particularly the multiplying of the square root) is called \"He Initialization\"\n",
    "            # It is known to work well for reLU activation.\n",
    "            weights = np.random.randn(nodes[i], nodes[i-1]) * np.sqrt(2 / nodes[i-1])       \n",
    "            b = np.zeros((nodes[i], 1))\n",
    "            \n",
    "        layer_weights.append(weights)\n",
    "        layer_biases.append(b)\n",
    "         \n",
    "    return layer_weights, layer_biases\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d97cc4",
   "metadata": {},
   "source": [
    "# Part 3 - Activation Functions\n",
    "### Activation functions are a crucial part of a neural network. They introduce non-linearities into the model. These non-linearities are what allow the neural network to fit to non-linear curves, which is what makes them useful. There are many activation functions, each of which have their proper use cases. Here, I use ReLU for the hidden layers, and Softmax for the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59c4f0c",
   "metadata": {},
   "source": [
    "### ReLU Activation\n",
    "#### ReLU activation (Rectified Linear Unit) is a function that takes as input the result of $z = wx + b$, (weights multiplied by inputs with an added bias layer) and returns either the value of $z$ or $0$, whichever is greater. This has become the go-to activation function for hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "284782fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reLU(matrix):\n",
    "    \"\"\" Computes the reLU activation elementwise on the matrix\"\"\"\n",
    "    \"\"\"Input: a numpy array\n",
    "       Output: the reLu activation applied to each element\"\"\"\n",
    "        \n",
    "    matrix = np.maximum(0, matrix) # Choose between the matrix element, or 0, whichever is greater\n",
    "    \n",
    "    return matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8022c50",
   "metadata": {},
   "source": [
    "### Softmax Activation\n",
    "#### Softmax Activation is a function that is used on the output layer of multi-class classification applications. The purpose is to return a relative probability with respect to every other class. This means that the sum of the results of softmax equals 1. This means that the prediction of the output class is the index of value with the highest number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b165febf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(array):\n",
    "    \"\"\"Computes the softmax value elementwise in an array\"\"\"\n",
    "    \"\"\"Input: a numpy array\n",
    "       Output: a numpy array of the same size as Input\"\"\"\n",
    "    \n",
    "    soft_array = np.exp(array) / np.sum(np.exp(array), axis=0) # axis=0 indicates summing over rows (summing all the elements in a column)\n",
    "    \n",
    "    return soft_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d688162e",
   "metadata": {},
   "source": [
    "# Part 4  - Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e209df",
   "metadata": {},
   "source": [
    "### Forward propagation is the algorithm that takes an input, runs it through the network and makes a prediction. There is a basic form of this algorithm that operates on a single layer, and the output is passed as input to the subsequent layer. Here I implement a forward propagation for a single layer.\n",
    "\n",
    "$$Z = Wx + b$$\n",
    "$$A = f(Z)$$\n",
    "#### Where f(x) is the activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe8da3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(weights, bias, layer_input, activation='relu'):\n",
    "    \"\"\"Implements a single step of forward propagation\"\"\"\n",
    "    \"\"\"Inputs\n",
    "            weights: array of weights for this layer\n",
    "            bias: vector of bias values for this layer\n",
    "            layer_input: the input to this layer\n",
    "            activation_function: the activation to be used on this layer - 'relu' by default\n",
    "       Outputs\n",
    "            Z: the result of Wx + b (needed for back propagation)\n",
    "            activation: The activated result of Wx + b\"\"\"\n",
    "    \n",
    "    Z = np.dot(weights, layer_input) + bias\n",
    "    \n",
    "    if activation == 'relu':\n",
    "        A = reLU(Z)\n",
    "    elif activation == 'softmax':\n",
    "        A = softmax(Z)\n",
    "        \n",
    "    return A , Z\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67b1699c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = [np.shape(X_train)[0], 30, 20, 10]\n",
    "weights, biases = initialize_parameters(nodes)\n",
    "A1, Z1 = forward_propagation(weights[1], biases[1], X_train, activation='relu')\n",
    "A2, Z2 = forward_propagation(weights[2], biases[2], A1, activation='relu')\n",
    "A3, Z3 = forward_propagation(weights[3], biases[3], A2, activation='softmax')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0735643e",
   "metadata": {},
   "source": [
    "# Part 5 - Cost Function\n",
    "### The cost function is a function that sums the differences between every value the model predicts, and its true value. It is this cost function we are trying to minimize. When the predictions are really small, the cost value will also be small. I use a function called categorical cross entropy. The total function (summed over all examples) is defined as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91169bfb",
   "metadata": {},
   "source": [
    "$$ J = -(1/m)\\Sigma_{j=0}^{M}\\Sigma_{i=0}^{N}(y_{i} * log(a_{ij}))$$\n",
    "\n",
    "Where M = Number of examples and N = Number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75679549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_cost(y_true, y_predicted):\n",
    "    \"\"\"Calculate the cross entropy loss\"\"\"\n",
    "    \"\"\"Inputs\n",
    "            y_true: Array of the true labels\n",
    "            y_predicted: Array of the predicted labels\n",
    "            \n",
    "       Outputs\n",
    "               J: the cross entropy loss (a single value)\"\"\"\n",
    "    \n",
    "    # Find the number of examples\n",
    "    m = np.shape(y_true)[1]\n",
    "    # Calculate the log of y_predicted\n",
    "    log_y_pred = np.log(y_predicted)\n",
    "    \n",
    "    # Multiply by the true labels\n",
    "    product = y_true * log_y_pred\n",
    "    \n",
    "    # sum across rows and columns\n",
    "    J = -np.sum(product)\n",
    "    \n",
    "    return J / m\n",
    "    \n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91f5d11",
   "metadata": {},
   "source": [
    "# Part 6 - Backpropagation\n",
    "\n",
    "### Backpropagation is the the heart and soul of a Neural Network. Backpropagation is the process by which the weights and biases are determined. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4925756",
   "metadata": {},
   "source": [
    "## The Math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2a9e95",
   "metadata": {},
   "source": [
    "### For a single input example, the loss or error can be expressed using categorical cross entropy, which was seen briefly above as part of the overall cost function. Here it is again all on it's own:\n",
    "$$ L = \\Sigma_{i=0}^{N}(y_{i} * log(a_{i}))$$\n",
    "#### Where N = # of Classes\n",
    "\n",
    "### The goal is to differentiate the loss function with respect to the adjustable parameters, $ W_l, b_l$ where $ l $ is the $l^{th}$ layer of the network. This is a process that requires extensive use of the chain rule, and also the derivation of the Softmax function. The following videos from Coding Lane and ML Dawn are extremely helpful:\n",
    "[Coding Lane](https://www.youtube.com/watch?v=f-nW8cSa_Ec) <br>\n",
    "[ML Dawn](https://www.youtube.com/watch?v=znqbtL0fRA0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18573a20",
   "metadata": {},
   "source": [
    "### The reason for the extensive use of the chain rule is due to the fact that the loss function $L$ is a function of $a_i$, but $a_i$ is a function of $z_i$, which is a function of $W$ and $b$. And that is only for one layer. $z_i$ is a function of the previous layers activation, $a_{i_{prev}}$, so the process must continue all the way back to the first layers parameters $W_1$ and $b_1$. For a 3 layer network, here are all of the derivatives of L with respect to the weights and biases of each layer expressed in partial derivatives:\n",
    "\n",
    "#### Note: The subscripts used here, i.e., $a_{l}, W_{l}$, etc, indicate the $l^{th}$ layer of $a$, not the index.\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W_3} = \\frac{\\partial L}{\\partial a_3}\\frac{\\partial a_3}{\\partial z_3}\\frac{\\partial z_3}{\\partial W_3} $ <br><br>\n",
    "$\\frac{\\partial L}{\\partial b_3} = \\frac{\\partial L}{\\partial a_3}\\frac{\\partial a_3}{\\partial z_3}\\frac{\\partial z_3}{\\partial b_3}$<br><br><br>\n",
    "$\\frac{\\partial L}{\\partial W_2} = \\frac{\\partial L}{\\partial a_3}\\frac{\\partial a_3}{\\partial z_3}\\frac{\\partial z_3}{\\partial a_2}\\frac{\\partial a_2}{\\partial z_2}\\frac{\\partial z_2}{\\partial W_2}$<br><br>\n",
    "$\\frac{\\partial L}{\\partial b_2} = \\frac{\\partial L}{\\partial a_3}\\frac{\\partial a_3}{\\partial z_3}\\frac{\\partial z_3}{\\partial a_2}\\frac{\\partial a_2}{\\partial z_2}\\frac{\\partial z_2}{\\partial b_2}$<br><br><br>\n",
    "$\\frac{\\partial L}{\\partial W_1} = \\frac{\\partial L}{\\partial a_3}\\frac{\\partial a_3}{\\partial z_3}\\frac{\\partial z_3}{\\partial a_2}\\frac{\\partial a_2}{\\partial z_2}\\frac{\\partial z_2}{\\partial a_1}\\frac{\\partial a_1}{\\partial z_1}\\frac{\\partial z_1}{\\partial W_1}$<br><br>\n",
    "$\\frac{\\partial L}{\\partial b_1} = \\frac{\\partial L}{\\partial a_3}\\frac{\\partial a_3}{\\partial z_3}\\frac{\\partial z_3}{\\partial a_2}\\frac{\\partial a_2}{\\partial z_2}\\frac{\\partial z_2}{\\partial a_1}\\frac{\\partial a_1}{\\partial z_1}\\frac{\\partial z_1}{\\partial b_1}$<br><br><br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6944c0dd",
   "metadata": {},
   "source": [
    "## Simplifying"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d329e6b",
   "metadata": {},
   "source": [
    "### Fortunately, there are more concise ways to write these equations. One can see that $\\frac{\\partial L}{\\partial z_l}$ terms are in each equation. Also, $\\frac{\\partial z_l}{\\partial W_l} = a_{(l-1)}$ and $\\frac{\\partial z_l}{\\partial b_l} = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b637ad",
   "metadata": {},
   "source": [
    "$\\frac{\\partial L}{\\partial W_3} = \\frac{\\partial L}{\\partial z_3}a_2$<br><br>\n",
    "$\\frac{\\partial L}{\\partial b_3} = \\frac{\\partial L}{\\partial z_3}$<br><br><br>\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W_2} = \\frac{\\partial L}{\\partial z_2}a_1$<br><br>\n",
    "$\\frac{\\partial L}{\\partial b_2} = \\frac{\\partial L}{\\partial z_2}$<br><br><br>\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W_1} = \\frac{\\partial L}{\\partial z_1}a_0$ (a0 = input vector)<br><br>\n",
    "$\\frac{\\partial L}{\\partial b_1} = \\frac{\\partial L}{\\partial z_1}$<br><br><br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ade609b",
   "metadata": {},
   "source": [
    "## Simplifying Further..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f915a682",
   "metadata": {},
   "source": [
    "### At this point the equations still expressed in terms of derivatives. This issue is addressed by evaluating $\\frac{\\partial L}{\\partial z_3}$.  Because this derivative involves the derivative of the softmax function, this term evalutes to $\\frac{\\partial L}{\\partial W_3} = (a - y)$ where $a$ is the predicted value output of softmax (in this case a vector), and $y$ is the vector with the correct one-hot label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0bacf4",
   "metadata": {},
   "source": [
    "### At this point a pattern begins to emerge. It can be seen that $\\frac{\\partial L}{\\partial z_2}$ can be expressed in terms of $\\frac{\\partial L}{\\partial z_3}$, and $\\frac{\\partial L}{\\partial z_1}$ in terms of $\\frac{\\partial L}{\\partial z_2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cfd713",
   "metadata": {},
   "source": [
    "$\\frac{\\partial L}{\\partial z_2} = \\frac{\\partial L}{\\partial z_3}W_3\\frac{\\partial a_2}{\\partial z_2}$<br><br>\n",
    "$\\frac{\\partial L}{\\partial z_1} = \\frac{\\partial L}{\\partial z_2}W_2\\frac{\\partial a_1}{\\partial z_1}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec988802",
   "metadata": {},
   "source": [
    "### It turns out for all hidden layers a general pattern can be expressed:\n",
    "$$\\frac{\\partial L}{\\partial z_l} = \\frac{\\partial L}{\\partial z_{l+1}}W_{l+1}\\frac{\\partial a_l}{\\partial z_l}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52989259",
   "metadata": {},
   "source": [
    "### To make this a little more readable, remember that $\\frac{\\partial a_l}{\\partial z_l}$ is the derivative of the activation function of the hidden layers, in this case ReLU. Which means if we change the form to $f'(z_l)$, then this general pattern for the hidden layers becomes:\n",
    "$$\\frac{\\partial L}{\\partial z_l} = \\frac{\\partial L}{\\partial z_{l+1}}W_{l+1} * f'(z_l)$$\n",
    "\n",
    "#### The * indicates element-wise multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1142a6b",
   "metadata": {},
   "source": [
    "## Bringing it all together...\n",
    "\n",
    "### Remember that all of these equations are for a single input example. The last thing that needs to be done is to express all of this in terms of the total cost function $J$, or across the whole dataset. The results are the vectorized versions of these equations.\n",
    "\n",
    "### For the final layer ($F$): \n",
    "$$ \\frac{\\partial J}{\\partial Z_F} = A_F - Y $$<br>\n",
    "$$ \\frac{\\partial J}{\\partial W_F} = \\frac{1}{m}(A_F - Y)A_{F-1}^{T}$$<br>\n",
    "$$ \\frac{\\partial J}{\\partial b_F} = \\frac{1}{m}sum((A_F - Y), axis = 1)$$<br><br>\n",
    "\n",
    "### For all hidden layers ($l$):\n",
    "$$\\frac{\\partial J}{\\partial Z_l} = W_{l+1}^T\\frac{\\partial J}{\\partial Z_{l+1}} * f'(Z_l)$$<br>\n",
    "$$\\frac{\\partial J}{\\partial W_l} = \\frac{1}{m}\\frac{\\partial J}{\\partial Z_{l+1}}A_{l-1}^{T}$$<br>\n",
    "$$\\frac{\\partial J}{\\partial b_l} = \\frac{1}{m}sum(\\frac{\\partial J}{\\partial Z_{l+1}}, axis=1)$$<br><br>\n",
    "#### Note: In order for matrix multiplication to work in $\\frac{\\partial J}{\\partial Z_l}$,   the weight matrix $W_{l+1}$ must be transposed and come before $\\frac{\\partial J}{\\partial Z_{l+1}}$.\n",
    "#### Note: The reason for the sum in the gradient with respect to b is because every column is the gradient of a single example, so all must be added, hence $axis=1$ (a pythonic way of expressing this sum).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47dba5f",
   "metadata": {},
   "source": [
    "## The Derivative of ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223e841b",
   "metadata": {},
   "source": [
    "### There is one more thing needed to implement these equations, and that is the derivative of relu, which fortunately is quite simple.\n",
    "\n",
    "#### If $ReLU(x) = f(x)$, then $f'(x) = \\begin{cases}\n",
    "1&\\text{if}&x > 0\\\\\n",
    "0&\\text{if}&x \\leq 0\\\\\n",
    "\\end{cases}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbe2940f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_ReLU(array):\n",
    "    \"\"\"Calculates the derivative of ReLU for every element in array\"\"\"\n",
    "    \"\"\"Input:array\n",
    "       Output: the elementwise derivative of array\"\"\"\n",
    "    \n",
    "    array = np.heaviside(array, 0)\n",
    "    \n",
    "    return array\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6033dbc9",
   "metadata": {},
   "source": [
    "## Backpropagation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb9f6c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(dJZ_prev, W_prev, Z, A_prev, m):\n",
    "    \"\"\"Computes one step of backward propagation\"\"\"\n",
    "    \"\"\"Input:\n",
    "            dJZ_prev: derivative wrt Z of the previous layer\n",
    "            W_prev: the weight matrix of the previous layer\n",
    "            A_prev\n",
    "            Z:  the pre-activated output of the current layer\n",
    "            m: the number of examples \n",
    "            \n",
    "       Output:\n",
    "             dJZ - gradient wrt Z of the current layer\n",
    "             dJW - gradient wrt W of the current layer\n",
    "             dJb - gradient wrt b of the current layer\"\"\"\n",
    "    \n",
    "    \n",
    "    dJZ = (1 / m) * np.dot(np.transpose(W_prev), dJZ_prev) * derivative_ReLU(Z)\n",
    "    dJW = (1 / m) * np.dot(dJZ, np.transpose(A_prev))\n",
    "    dJb = (1 / m) * np.sum(dJZ, axis=1, keepdims=True)\n",
    "    \n",
    "    return dJZ, dJW, dJb\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43840a72",
   "metadata": {},
   "source": [
    "## Preparing the Data into Batches for Batch Gradient Descent\n",
    "#### This method of training the data requires the input to be split into smaller batches. The purpose is to help the network train faster. This works because it is easier for the hardware to compute a gradient descent step with a smaller portion of the data as opposed to entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e15c7bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_batches = []\n",
    "y_train_batches = []\n",
    "\n",
    "batch_size = 1024 #This number is chosen as computer hardware scales by factors of 2.(2^10)\n",
    "num_batches = int(np.ceil(X_train.shape[1] / batch_size))\n",
    "for i in range(num_batches):\n",
    "    if i == num_batches - 1:\n",
    "        x_batch_i = X_train[:, batch_size * i :]\n",
    "        y_batch_i = Y_train_oh[:, batch_size * i :] \n",
    "    else:   \n",
    "        x_batch_i = X_train[:, batch_size * i : batch_size * (i+1)]\n",
    "        y_batch_i = Y_train_oh[:, batch_size * i : batch_size * (i+1)]\n",
    "\n",
    "    x_train_batches.append(x_batch_i)\n",
    "    y_train_batches.append(y_batch_i)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e97ab02",
   "metadata": {},
   "source": [
    "## Part 7 - The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ea9ca4",
   "metadata": {},
   "source": [
    "### The model introduces a couple of new concepts. Epochs, and the learning rate, alpha. Epochs is the number of times the entire dataset is run through the network. Alpha ($\\alpha$) is a scaling factor for the derivatives before they are subtracted from the previous parameter values.\n",
    "\n",
    "$$ W_l = W_l - \\alpha \\frac{\\partial J}{\\partial W_l}$$<br>\n",
    "$$ b_l = b_l - \\alpha \\frac{\\partial J}{\\partial b_l}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290e40e6",
   "metadata": {},
   "source": [
    "### When the model is ran, it will compute the weights and biases that decrease the total cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2d8e6404",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network(arch, X, Y, X_test, Y_test, epochs, alpha):\n",
    "    \"\"\"Solves for and returns the weights of the neural network\"\"\"\n",
    "    \"\"\"Input:\n",
    "            arch: A vector of length l where l is the number of layers and the values\n",
    "                            are the number of nodes\n",
    "            X: The input array (a list of np arrays)\n",
    "            Y: The label array (a list of np arrays)\n",
    "            X_test: The test data array\n",
    "            Y_test: The test label array\n",
    "            epochs: # of epochs to train on\n",
    "            alpha: Learning rate\n",
    "       Output: The optimized parameters W and b.\"\"\"\n",
    "    \n",
    "    # The number of examples\n",
    "    # m = np.shape(X)[1] \n",
    "    \n",
    "    # Initialize the parameters\n",
    "    W, b = initialize_parameters(arch)\n",
    "    \n",
    "    # Initialize cost vector\n",
    "    J_train = np.zeros(epochs)\n",
    "    J_test = np.zeros(epochs)\n",
    "    \n",
    "    # Run forward and backprop \n",
    "    for i in range(epochs): # Loop through epochs (entire data set)\n",
    "        for j in range(len(X)): # Loop through batches\n",
    "        # Find the number examples in batch j\n",
    "            m_batch = np.shape(X[j])[1]\n",
    "            # Perform forward propagation through on the training set.\n",
    "            A1, Z1 = forward_propagation(W[1], b[1], X[j], activation='relu')\n",
    "            A2, Z2 = forward_propagation(W[2], b[2], A1, activation='relu')\n",
    "            A3, Z3 = forward_propagation(W[3], b[3], A2, activation='softmax')\n",
    "\n",
    "            # Perform forward propgation on the test set (to see how well the network performs on this set of parameters).\n",
    "            A1_test, Z1_test = forward_propagation(W[1], b[1], X_test, activation='relu')\n",
    "            A2_test, Z2_test = forward_propagation(W[2], b[2], A1_test, activation='relu')\n",
    "            A3_test, Z3_test = forward_propagation(W[3], b[3], A2_test, activation='softmax')\n",
    "\n",
    "            # Compute the cost for this epoch\n",
    "            J_train_i = cross_entropy_cost(Y[j], A3)\n",
    "            J_train[i] = J_train_i\n",
    "            J_test_i = cross_entropy_cost(Y_test_oh, A3_test)\n",
    "            J_test[i] = J_test_i\n",
    "\n",
    "\n",
    "            # Compute the gradients for the final layer ------------> transposing may be an issue\n",
    "            dJZ3 = A3 - Y[j]\n",
    "            dJW3 = (1 / m_batch) * np.dot((A3 - Y[j]), np.transpose(A2))\n",
    "            dJb3 = (1 / m_batch) * np.sum((A3 - Y[j]), axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "            # Compute the gradients for the hidden layers\n",
    "            dJZ2, dJW2, dJb2 = backward_propagation(dJZ3, W[3], Z2, A1, m_batch)\n",
    "            dJZ1, dJW1, dJb1 = backward_propagation(dJZ2, W[2], Z1, X[j], m_batch)\n",
    "\n",
    "            # Update the parameters\n",
    "            W[3] -= alpha * dJW3\n",
    "            b[3] -= alpha * dJb3\n",
    "            W[2] -= alpha * dJW2\n",
    "            b[2] -= alpha * dJb2\n",
    "            W[1] -= alpha * dJW1\n",
    "            b[1] -= alpha * dJb1\n",
    "\n",
    "    return J_train, J_test, W, b\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e8e50948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time: 71.82362914085388\n"
     ]
    }
   ],
   "source": [
    "X = x_train_batches\n",
    "Y = y_train_batches\n",
    "arch = [np.shape(X[0])[0], 128, 64, 10] # For the purposes of this notebook, this must be 4 elements long and the last value must be 10.\n",
    "epochs = 10\n",
    "alpha = 1.2 # 5 is unusual for a learning rate. Typically they range between 10^-6 and 1.\n",
    "\n",
    "# Run and time the training of the network\n",
    "start = time.time()\n",
    "train_cost, test_cost, W_solved, b_solved = neural_network(arch, X, Y, X_test, Y_test_oh, epochs, alpha)\n",
    "end = time.time()\n",
    "print(f'Train time: {end-start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1478046d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(epochs), train_cost)\n",
    "plt.plot(np.arange(epochs), test_cost)\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Loss Curve')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfc07e7",
   "metadata": {},
   "source": [
    "### Here is the Loss Curve. This curve shows how the cost is affected as the amount of time/experience the network has increases. The training curve is less than the test curve which is expected. They are both decreasing, but beginning to level out, indicating that the training time is approaching its optimum.\n",
    "\n",
    "#### In a separate notebook I implemented a this same algorithm without batch gradient descent. Not only does it take more than 3 times as long to train (while taking fewer steps) the accuracy is less than that with batch gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0f09ff",
   "metadata": {},
   "source": [
    "## Part 8 - Testing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d53c243",
   "metadata": {},
   "source": [
    "### Now that a model has been generated, its accuracy must be determined against both the training set and the test set. Ideally, the training set should have higher accuracy than the test set since it was trained to fit the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e5f4cf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(X, W, b):\n",
    "    \"\"\"Calculate the predictions of the network\"\"\"\n",
    "    \"\"\"Input: \n",
    "            X: train/test dataset\n",
    "            W: solved weights\n",
    "            b: solved biases\n",
    "            \n",
    "       Returns:\n",
    "            predictions: the predictions of the network\"\"\"\n",
    "    \n",
    "    # Run the dataset forward through the network.\n",
    "    A1, Z1 = forward_propagation(W[1], b[1], X, activation='relu')\n",
    "    A2, Z2 = forward_propagation(W[2], b[2], A1, activation='relu')\n",
    "    A3, Z3 = forward_propagation(W[3], b[3], A2, activation='softmax')\n",
    "    \n",
    "    # Find the prediction (the index value of the max value in each column).\n",
    "    pred = np.argmax(A3, axis=0)\n",
    "    \n",
    "    return pred\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e03b16b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the predictions of the model on the train and test set\n",
    "train_predict = prediction(X_train, W_solved, b_solved)\n",
    "test_predict = prediction(X_test, W_solved, b_solved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b417f568",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Accuracy is 71.77 %\n",
      "Test Set Accuracy is 72.37 %\n"
     ]
    }
   ],
   "source": [
    "# Determine how many of each set was accurate.\n",
    "a = (train_predict == Y_train)\n",
    "b = (test_predict == Y_test)\n",
    "train_accuracy = np.sum(a) / len(Y_train)\n",
    "test_accuracy = np.sum(b) / len(Y_test)\n",
    "print(f'Training Set Accuracy is {train_accuracy*100:.2f} %')\n",
    "print(f'Test Set Accuracy is {test_accuracy*100:.2f} %')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bee03b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 792    6   40   15   12   26   11   33   12   33]\n",
      " [   0 1037   29    4    4   27    4    7   17    6]\n",
      " [  32   45  644   75   28   30   68   30   54   26]\n",
      " [   8   31   49  715   16   66   21   48   25   31]\n",
      " [  10   25   23   29  658   35   59   26   18   99]\n",
      " [  29   42   48  109   56  331   37   76  103   61]\n",
      " [  40   25   59    2   40   35  698   13   19   27]\n",
      " [  35   47   39   18   16    9   19  744   30   71]\n",
      " [  29   55   61   55   46   46   31   43  569   39]\n",
      " [  34   40   13   33  126   30   50  131   47  505]]\n"
     ]
    }
   ],
   "source": [
    "# Compare labels to their predictions.\n",
    "label_and_prediction = np.zeros((10, 10), dtype=np.int16)\n",
    "\n",
    "for i in range(len(Y_test)):\n",
    "    label_and_prediction[Y_test[i], test_predict[i]] += 1\n",
    "\n",
    "print(label_and_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cce2483",
   "metadata": {},
   "source": [
    "## Part 9 - Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0dcab8",
   "metadata": {},
   "source": [
    "### This was a great learning experience. First I prepared the dataset to be acceptable to a neural network. Then I created activation functions. Next came the forward and backward propgation functions along with the cross entropy function. Afterward it all came together to solve a neural network with many times better accuracy than random guessing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44898008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    This image is labeled as a \"3\"\n",
      "\n",
      "    This image is predicted as a 3\n"
     ]
    }
   ],
   "source": [
    "# Look at an image from the test set, see its label and prediction.\n",
    "ex_img = 5212\n",
    "image = X_tst[ex_img]\n",
    "fig = plt.figure\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()\n",
    "print(f'    This image is labeled as a \"{Y_tst[ex_img]}\"\\n')\n",
    "print(f'    This image is predicted as a {test_predict[ex_img]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fee557",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
